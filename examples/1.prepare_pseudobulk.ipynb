{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook makes the Pseudobulks from processed single-cell data and formats relevant metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import buddi preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = subprocess.run(\n",
    "    [\"git\", \"rev-parse\", \"--show-toplevel\"], capture_output=True, text=True\n",
    ").stdout.strip()\n",
    "REPO_ROOT = pathlib.Path(REPO_ROOT)\n",
    "BUDDI_PATH = REPO_ROOT / \"src\"\n",
    "\n",
    "sys.path.insert(0, str(BUDDI_PATH))\n",
    "\n",
    "from buddi_v2 import preprocessing\n",
    "from buddi_v2.preprocessing import utils\n",
    "from buddi_v2.preprocessing import generate_pseudo_bulks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'liver'\n",
    "\n",
    "CELL_TYPE_COL = 'scpred_CellType'\n",
    "CELL_TYPE_COL_SOURCE = 'names_merged'\n",
    "SAMPLE_ID_COL = 'sample_id'\n",
    "SAMPLE_ID_COL_SOURCE = 'mouse.id'\n",
    "STIM_COL = 'stim'\n",
    "STIM_COL_SOURCE = 'sex'\n",
    "def get_stim_id(in_str):\n",
    "    out_str = \"female\"\n",
    "    if in_str == \"male\":\n",
    "        out_str = \"male\"\n",
    "           \n",
    "    return(out_str)\n",
    "\n",
    "GENE_ID_COL = 'gene_ids'\n",
    "DATASPLIT_COL = 'isTraining'\n",
    "DATASPLIT_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input data path\n",
    "DATA_PATH = pathlib.Path('.').absolute() / 'example_data'\n",
    "assert DATA_PATH.exists()\n",
    "SC_DATA_FILE = DATA_PATH / 'processed_sc_liver' / 'liver_droplet_processed.h5ad'\n",
    "assert SC_DATA_FILE.exists()\n",
    "\n",
    "## Cibersortx\n",
    "CIBERSORTX_SIG_GENE_FILE = DATA_PATH / 'cibersort_liver' /\\\n",
    "    'CIBERSORTx_Job20_all-liver_0_cybersort_sig_inferred_phenoclasses.CIBERSORTx_Job20_all-liver_0_cybersort_sig_inferred_refsample.bm.K999.txt'\n",
    "assert CIBERSORTX_SIG_GENE_FILE.exists()\n",
    "\n",
    "## Output path\n",
    "PREPROCESS_OUTPUT_PATH = pathlib.Path('.').absolute() / 'example_data' / 'preprocessed_data'\n",
    "assert PREPROCESS_OUTPUT_PATH.exists(), 'Please create the output directory \"preprocessed_data\" first'\n",
    "PREPROCESS_SC_AUGMENTED_OUTPUT_PATH = PREPROCESS_OUTPUT_PATH / 'sc_augmented'\n",
    "PREPROCESS_SC_AUGMENTED_OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Single Cell Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(SC_DATA_FILE)\n",
    "adata.var_names_make_unique()\n",
    "\n",
    "if CELL_TYPE_COL_SOURCE not in adata.obs.columns:\n",
    "    raise ValueError(f\"Column {CELL_TYPE_COL_SOURCE} not found in the AnnData object.\")\n",
    "else:\n",
    "    adata.obs[CELL_TYPE_COL] = adata.obs[CELL_TYPE_COL_SOURCE].tolist()\n",
    "if SAMPLE_ID_COL_SOURCE not in adata.obs.columns:\n",
    "    raise ValueError(f\"Column {SAMPLE_ID_COL_SOURCE} not found in the AnnData object.\")\n",
    "else:\n",
    "    adata.obs[SAMPLE_ID_COL] = adata.obs[SAMPLE_ID_COL_SOURCE].tolist()\n",
    "if STIM_COL_SOURCE not in adata.obs.columns:\n",
    "    raise ValueError(f\"Column {STIM_COL_SOURCE} not found in the AnnData object.\")\n",
    "else:\n",
    "    adata.obs[STIM_COL] = [get_stim_id(str(x)) for x in adata.obs[STIM_COL_SOURCE].tolist()]\n",
    "\n",
    "adata.obs[DATASPLIT_COL] = \"Train\"\n",
    "stim_idx = np.where(adata.obs[STIM_COL] == \"female\")[0]\n",
    "adata.obs.loc[adata.obs.index[stim_idx], DATASPLIT_COL] = \"Test\"\n",
    "\n",
    "adata.var[GENE_ID_COL] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment sc Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cibersortx signature genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2194 signature genes from CIBERSORTx\n"
     ]
    }
   ],
   "source": [
    "cibersortx_sig_df = pd.read_csv(CIBERSORTX_SIG_GENE_FILE, sep='\\t', header=0)\n",
    "cibersortx_sig_genes = cibersortx_sig_df['NAME'].values.tolist()\n",
    "print(f\"{len(cibersortx_sig_genes)} signature genes from CIBERSORTx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Gene ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_out_file = PREPROCESS_SC_AUGMENTED_OUTPUT_PATH / f'{DATA_NAME}_genes.pkl'\n",
    "gene_ids = adata.var[GENE_ID_COL]\n",
    "pickle.dump(gene_ids, open( gene_out_file, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pseudo-bulks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_PER_CELL_TYPE_NOISE = False\n",
    "N_CELLS_PER_PSEUDO_BULK = 5_000\n",
    "N_PSEUDO_BULKS_PER_CONDITION = 1_000\n",
    "\n",
    "gene_ids = adata.var[GENE_ID_COL]\n",
    "samples = adata.obs[SAMPLE_ID_COL].unique()\n",
    "stims = adata.obs[STIM_COL].unique()\n",
    "\n",
    "cell_types = adata.obs[CELL_TYPE_COL].unique() # Global cell types\n",
    "cell_order = sorted(cell_types) # order proportion columns\n",
    "\n",
    "datasplits = adata.obs[DATASPLIT_COL].unique()\n",
    "\n",
    "n_samples = len(samples)\n",
    "n_genes = len(gene_ids)\n",
    "n_cell_types = len(cell_order)\n",
    "\n",
    "# Define cell-type level noise for the generated pseudo-bulk profiles\n",
    "if ADD_PER_CELL_TYPE_NOISE:\n",
    "    # this produces a list of numpy arrays, each of length n_genes\n",
    "    # to reflect the expression noise associated with each specific cell type\n",
    "    per_cell_type_noise = [\n",
    "        np.random.lognormal(0, 1, n_genes) for i in range(n_cell_types)]\n",
    "else:\n",
    "    per_cell_type_noise = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pseudo-bulk profiles for sample 18-F-51, stim female, and datasplit Test ...\n",
      ">Generating random prop pseudo-bulk profiles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 48.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Generating single cell dominant pseudo-bulk profiles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:17<00:00, 45.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Concatenating the two types of pseudo-bulk profiles ...\n",
      ">Writing the pseudo-bulk profiles ...\n",
      "Done for sample 18-F-51, stim female, and datasplit Test\n",
      "\n",
      "Generating pseudo-bulk profiles for sample 18-F-51, stim female, and datasplit Train ...\n",
      "Generating pseudo-bulk profiles for sample 18-F-51, stim male, and datasplit Test ...\n",
      "Generating pseudo-bulk profiles for sample 18-F-51, stim male, and datasplit Train ...\n",
      "Generating pseudo-bulk profiles for sample 30-M-5, stim female, and datasplit Test ...\n",
      "Generating pseudo-bulk profiles for sample 30-M-5, stim female, and datasplit Train ...\n",
      "Generating pseudo-bulk profiles for sample 30-M-5, stim male, and datasplit Test ...\n",
      "Generating pseudo-bulk profiles for sample 30-M-5, stim male, and datasplit Train ...\n",
      ">Generating random prop pseudo-bulk profiles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Generating single cell dominant pseudo-bulk profiles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:18<00:00, 43.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Concatenating the two types of pseudo-bulk profiles ...\n",
      ">Writing the pseudo-bulk profiles ...\n",
      "Done for sample 30-M-5, stim male, and datasplit Train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate pseudo-bulk profiles grouping by sample_id and stim\n",
    "for _sample in samples:\n",
    "    for _stim in stims:\n",
    "        for _datasplit in datasplits:\n",
    "\n",
    "            print(f\"Generating pseudo-bulk profiles for sample {_sample}, stim {_stim}, and datasplit {_datasplit} ...\")\n",
    "            \n",
    "            ## Subset adata to the current sample, stim and train/test split\n",
    "            subset_idx = np.where(\n",
    "                np.logical_and.reduce((\n",
    "                    adata.obs[SAMPLE_ID_COL] == _sample, \n",
    "                    adata.obs[STIM_COL] == _stim,\n",
    "                    adata.obs[DATASPLIT_COL] == _datasplit,\n",
    "                ))\n",
    "            )[0]\n",
    "            \n",
    "            if len(subset_idx) == 0:\n",
    "                continue\n",
    "            subset_adata = adata[subset_idx, :]\n",
    "\n",
    "            ## Cell type that is present in the subset, will inform the \n",
    "            ## down stream workflow of potential missing cell types to skip\n",
    "            present_cell_types = subset_adata.obs[CELL_TYPE_COL].unique().tolist()\n",
    "\n",
    "            ## Subset the cell_df to the present cell types\n",
    "            cell_df = preprocessing.utils.subset_adata_by_cell_type(\n",
    "                subset_adata, \n",
    "                cell_type_col=CELL_TYPE_COL,\n",
    "                cell_order=cell_order\n",
    "            )\n",
    "\n",
    "            print(\">Generating random prop pseudo-bulk profiles ...\")\n",
    "\n",
    "            random_count_df = preprocessing.utils.generate_log_normal_counts(\n",
    "                cell_order=cell_order, \n",
    "                num_cells=N_CELLS_PER_PSEUDO_BULK, \n",
    "                num_samples=N_PSEUDO_BULKS_PER_CONDITION,\n",
    "                present_cell_types=present_cell_types\n",
    "            )\n",
    "            random_props_df = preprocessing.utils.generate_prop_from_counts(\n",
    "                random_count_df,\n",
    "            )\n",
    "            random_pseudobulk_df = preprocessing.generate_pseudo_bulks.generate_pseudo_bulk_from_counts(\n",
    "                in_adata=subset_adata,\n",
    "                cell_df=cell_df,\n",
    "                count_df=random_count_df,\n",
    "                cell_noise=per_cell_type_noise,\n",
    "                use_sample_noise=False\n",
    "            )\n",
    "            random_pseudobulk_metadata_df = pd.DataFrame(\n",
    "                data = {\n",
    "                    SAMPLE_ID_COL: [_sample]*N_PSEUDO_BULKS_PER_CONDITION,\n",
    "                    STIM_COL: [_stim]*N_PSEUDO_BULKS_PER_CONDITION,\n",
    "                    'cell_prop_type': ['random']*N_PSEUDO_BULKS_PER_CONDITION,\n",
    "                    'cell_type': ['random']*N_PSEUDO_BULKS_PER_CONDITION,\n",
    "                    'samp_type': ['sc_ref']*N_PSEUDO_BULKS_PER_CONDITION,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\">Generating single cell dominant pseudo-bulk profiles ...\")\n",
    "\n",
    "            single_cell_props_df, single_cell_metadata = preprocessing.utils.generate_single_celltype_dominant_props(\n",
    "                num_samp=100,\n",
    "                cell_order=cell_order,\n",
    "                present_cell_types=present_cell_types,\n",
    "                return_metadata=True\n",
    "            )\n",
    "            single_cell_counts_df = preprocessing.utils.generate_counts_from_props(\n",
    "                single_cell_props_df,\n",
    "                num_cells=N_CELLS_PER_PSEUDO_BULK\n",
    "            )\n",
    "            single_cell_pseudobulk_df = preprocessing.generate_pseudo_bulks.generate_pseudo_bulk_from_counts(\n",
    "                in_adata=subset_adata,\n",
    "                cell_df=cell_df,\n",
    "                count_df=single_cell_counts_df,\n",
    "                cell_noise=per_cell_type_noise,\n",
    "                use_sample_noise=False\n",
    "            )\n",
    "            n_single_celltype_pbs = len(single_cell_pseudobulk_df)\n",
    "            single_cell_metadata_df = pd.DataFrame(\n",
    "                data = {\n",
    "                    SAMPLE_ID_COL: [_sample]*n_single_celltype_pbs,\n",
    "                    STIM_COL: [_stim]*n_single_celltype_pbs,\n",
    "                    'cell_prop_type': ['single_celltype']*n_single_celltype_pbs,\n",
    "                    'cell_type': single_cell_metadata,\n",
    "                    'samp_type': ['sc_ref']*n_single_celltype_pbs,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print('>Concatenating the two types of pseudo-bulk profiles ...')\n",
    "            props_df = pd.concat([random_props_df, single_cell_props_df])\n",
    "            pseudobulk_df = pd.concat([random_pseudobulk_df, single_cell_pseudobulk_df])\n",
    "\n",
    "            metadata_df = pd.concat(\n",
    "                [random_pseudobulk_metadata_df, single_cell_metadata_df]\n",
    "            )\n",
    "            \n",
    "            print(\">Writing the pseudo-bulk profiles ...\")\n",
    "            pseudobulk_file = PREPROCESS_SC_AUGMENTED_OUTPUT_PATH / f'{DATA_NAME}_{_sample}_{_stim}_{_datasplit}_pseudo_splits.pkl'\n",
    "            prop_file = PREPROCESS_SC_AUGMENTED_OUTPUT_PATH / f'{DATA_NAME}_{_sample}_{_stim}_{_datasplit}_prop_splits.pkl'\n",
    "            meta_file = PREPROCESS_SC_AUGMENTED_OUTPUT_PATH / f'{DATA_NAME}_{_sample}_{_stim}_{_datasplit}_meta_splits.pkl'\n",
    "\n",
    "            pickle.dump( props_df, open( prop_file, \"wb\" ) )\n",
    "            pickle.dump( pseudobulk_df, open( pseudobulk_file, \"wb\" ) )\n",
    "            pickle.dump( metadata_df, open( meta_file, \"wb\" ) )\n",
    "\n",
    "            print(f\"Done for sample {_sample}, stim {_stim}, and datasplit {_datasplit}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buddi_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
